{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook serves as an example for the process of the analysis of semantic similarity. Semantic similarity requires not simply comparing text, but comparing what the text is supposed to mean. Even people can have disagreements over this, so computing something that abstract is a complex task.\n",
    "\n",
    "A concept of vital importance is text embedding. Text is relatively hard to compare; comparing numbers is significantly faster than comparing strings, the latter possibly requiring iterating over each character to check it. Transforming the text into a numerical representation brings a lot of efficiency, and is an important first step. There are multiple ways to go about this.\n",
    "\n",
    "The simplest way is called <i>bag of words</i>. The entire vocabulary of the library of text we want to compare is numerically allocated; each word is given an entry in a simple lookup table. A sentence can then be vectorized, each vector being the same length as the lookup table, and each index holding the count of how often the word assigned that number appears in the sentence. Comparing these vectors then essentially compares the overlap between words used in sentences, assuming that the same words lead to the same meanings.\n",
    "\n",
    "Of course, as the 'bag' implies, we lose order, a very important part of semantic meaning. Additionally, words may be functionally similar, but be different counts; two sentences each using different synonyms would be very different in this theory. 'Greater' could be stemmed to 'great', but the same is harder for 'better' and 'good'. What about 'terrific' and 'great'? We could look up synonyms for each word, but this quickly becomes very inefficient. Then there's the question of how relevant a word is; a 'I have a good book' is very different from a 'I have a good car', after all, despite being 80% \"similar\".\n",
    "\n",
    "This relevancy problem can be diminished using TFIDF, or term frequencyâ€“inverse document frequency, to introduce a weighting. The more often a term shows up in a document, which is any arbitrary amount of text, the more relevant it is. The more documents that term shows up in, however, the less relevant. The sentences above may be reduced to 'good book' and 'good car', for example, based on the exact implementation. \n",
    "\n",
    "A more complex solution is to use a continuous bag of words such as described in the Word2Vec model as originally patented by Google, an implementation of which exists in the open source Gensim package. The concept is similar to a normal bag of words, but instead of each word apart, we look at the surrounding words. Each word is paired with its surroundings, creating a structure that allows for the prediction of words: using the above sentences again, 'good' can predict 'a', 'book', or 'car'. Then, TFIDF could be applied to determine the odds of predicting each word, or these weights can be determined more accurately using a neural network, as described in the original article. The Gensim package uses the latter method. The (abstracted) weights can be vectorized, and these are then used much like the simpler embedding can be.\n",
    "\n",
    "To compare sentences, the embedded word vector can be taken for each word in the sentence, taking the average. Two averaged vectors can then be compared to determine how similar they are. This solves the relevancy and synonym problems, but does not preserve order; some context from sentence structure is lost. An expansion on the Word2Vec method, the Doc2Vec method, designed by the same authors, proposes to solve this by taking the word vectors in a document, and adding a 'phantom word'; the resulting so called document vector is then unique for that document, in an attempt to remember or at least account for the entire document context. This also has an implementation built in Gensim.\n",
    "\n",
    "How long the resulting vector is, depends on the implementation. The Gensim implementations use a vector size of 100 by default. Larger vectors can capture more details, but make computation more expensive. In our case, 100 was an apt choice \n",
    "\n",
    "In the next cell, we load versions of these three models trained on the data provided to us, to show their capability. Additionally, a pretrained Google model known as the Universal Sentence Encoder will be shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec, doc2vec\n",
    "from embed import universal_sentence_encoder as use_model\n",
    "\n",
    "\n",
    "# Load the pretrained models\n",
    "w2v_model = word2vec.Word2Vec.load(\"models/w2vmodel.mod\")\n",
    "d2v_model = doc2vec.Doc2Vec.load(\"models/doc2vec.model\")\n",
    "# tfidf model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "The above models and instruction clarify how to embed text, acquiring vectors for the relevant documents. It mentions comparing these vectors, but not exactly how. There are, again, multiple ways to do this. The simplest way is to calculate the cosine similarity between them, essentially imagining we have position vectors, and checking the angle between them. This is relatively easy to compute, intuitive, and gives a result between 0 and 1, making it very easy to use; set or train a proper threshold, and if the cosine similarity is above that threshold, the sentences carry the same semantic intent.\n",
    "\n",
    "A complexer but significantly more accurate way of comparing is through the use of a neural network. Starting with more-or-less random predictions, the correct answers are used to calculate how wrong the network's prediction was, which is then used to update it, improving the next prediction. Through repetition, the network thus learns how to tell which vectors are supposed to be 'similar', learning to classify them. We implemented a relatively simple neural network, that takes as input two concatenated vectors; the two sentence vectors we want to compare. One hidden layer, of equal size to the input layer, with as activation function the rectified linear activation function, and finally a sigmoid function for the output layer of size 1. This output is simply a 0 or 1, signifying whether the sentences are the same or not.\n",
    "\n",
    "The training of this second method is rather computationally expensive, though training doesn't necessarily have happen more than once. However, it also requires a large amount of annotated data; a large list of sentence combinations of which it is known if they are semantically the same or not. Additionally, this data should be very varied, as scenarios not adequately present in the training data will be hard to predict. This data being accurate is also of vital importance for accuracy, meaning it will have to be done, or at least verified, by hand. Our data consisted of about 400000 pairs of questions, meaning the network is likely to be better at classifying questions than other sentence structures. While this machine learning is not applicable in every situation, the prerequisites are something to keep in mind.\n",
    "\n",
    "Below we load our pretrained models, after which the requirements for analysis are present, and some examples can be run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "# Load the pretrained models\n",
    "w2v_network = keras.models.load_model('neuralnets/w2v.h5')\n",
    "d2v_network = keras.models.load_model('neuralnets/d2v.h5')\n",
    "use_network = keras.models.load_model('neuralnets/use.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifiers import binaryclassification as bc\n",
    "from embed import doc2vec\n",
    "from embed import w2vec\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy import spatial\n",
    "import tensorflow as tf\n",
    "tokenizer = RegexpTokenizer(r'\\w+\\'*\\w*')\n",
    "\n",
    "sentence1 = \"Is it possible for a meme to beat two memes?\"\n",
    "sentence2 = \"Is it possible for a meme to beat two memes?\"\n",
    "\n",
    "tokens1 = tokenizer.tokenize(sentence1.lower())\n",
    "tokens2 = tokenizer.tokenize(sentence2.lower())\n",
    "\n",
    "w2v_vector = np.array(bc.vectorize_w2v(w2v_model, tokens1, tokens2)).reshape((1, 200))\n",
    "d2v_vector = np.array(doc2vec.doc2vec(d2v_model, tokens1, tokens2)).reshape((1, 200))\n",
    "use_vector = tf.convert_to_tensor(np.array(tf.concat([use_model.encode(sentence1), use_model.encode(sentence2)], axis=0)).reshape((1, 1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2Vnet says: False \tW2V Cosine: True \tconclusion: True\n",
      "D2Vnet says: False \tD2V Cosine: False \tconclusion: False\n",
      "USEnet says: False \tUSE Cosine: True \tconclusion: True\n"
     ]
    }
   ],
   "source": [
    "w2vnet_ans = w2v_network.predict((w2v_vector) > 0.5).astype(\"int32\")[0][0] == 1\n",
    "w2vcos_ans = w2vec.string_similarity(w2v_model, tokens1, tokens2) > 0.98\n",
    "\n",
    "d2vnet_ans = d2v_network.predict((d2v_vector) > 0.5).astype(\"int32\")[0][0] == 1\n",
    "d2vcos_ans = (1 - spatial.distance.cosine(d2v_vector[0][:100], d2v_vector[0][100:])) > 0.98\n",
    "\n",
    "usenet_ans = use_network.predict((use_vector) > 0.5).astype(\"int32\")[0][0] == 1\n",
    "usecos_ans = (1 - spatial.distance.cosine(use_vector[0][:512], use_vector[0][512:])) > 0.98\n",
    "\n",
    "print(\"W2Vnet says:\", w2vnet_ans, \"\\tW2V Cosine:\", w2vcos_ans, \"\\tconclusion:\", w2vcos_ans or w2vnet_ans)\n",
    "print(\"D2Vnet says:\", d2vnet_ans, \"\\tD2V Cosine:\", d2vcos_ans, \"\\tconclusion:\", d2vnet_ans or d2vcos_ans)\n",
    "# print(\"TFIDF says: \", model_tfidf.predict(tfidf_vector))\n",
    "print(\"USEnet says:\", usenet_ans, \"\\tUSE Cosine:\", usecos_ans, \"\\tconclusion:\", usenet_ans or usecos_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
